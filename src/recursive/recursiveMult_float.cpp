#include <iostream>
#include <fstream>
#include <math.h>
#include <tuple>   					// C++11 class
#include <deque>
#include <vector>
#include <map>
#include <queue>
#include <sys/time.h>
#include <sys/resource.h>
#include <thread>  					// Using one of C++11 thread functions
#include <iomanip> 					// Output formatting
#include <pthread.h>
#include "Array2D.h"

using namespace std;

const int g_mortonBlockSide = 128;	// Block size for memory layout.
const int g_outputWidth = 9; 		// Set the output width for elements
const int g_outputPrecision = 15;	// Matrix C elements output precision.
int g_cacheBlockSize = 16;	// Block size for loop tilling/blocking. Must be multiple of two.

typedef float E; 					// Define the type of elements being used.

// Task object class.
class Task	
{
public:
	
	// Matrices starting indexes i and j and matrices dimensions.
	int m_A_ini_i;
	int m_A_ini_j;
	int m_A_dim1;
	int m_A_dim2;
	int m_B_ini_i;
	int m_B_ini_j;
	int m_B_dim2;
	int m_C_ini_i;
	int m_C_ini_j;
	int m_rlevel;	// Recursion level of this task.
	
	Task() { };
	
    Task(int A_ini_i, int A_ini_j, int A_dim1, int A_dim2, int B_ini_i, int B_ini_j, int B_dim2, int C_ini_i, int C_ini_j, int rlevel) 
    {
    	m_A_ini_i = A_ini_i;
    	m_A_ini_j = A_ini_j;
		m_A_dim1 = A_dim1;
		m_A_dim2 = A_dim2;
		m_B_ini_i = B_ini_i;
		m_B_ini_j = B_ini_j;
		m_B_dim2 = B_dim2;
		m_C_ini_i = C_ini_i;
		m_C_ini_j = C_ini_j;
		m_rlevel = rlevel;
    }
	
};

// TaskManager is used to assign tasks to threads.
class TaskManager	
{
public:
	int m_lastLevel; // Deepest recursion level.
	
	// Each thread has a task deque for different recursion levels. 
	std::vector < std::vector < std::deque <Task> > > taskVector;
	
	// Maps writing point C(i, j) to a thread ID.
	std::map < std::tuple<int,int>, int > taskMap;
	
    void initManager(int numThreads) {
    	taskMap.clear();
    	taskVector.clear();
    	// Each thread has a task deque for different recursion levels. 
    	taskVector.assign(numThreads, std::vector < std::deque<Task> > ());
    	for (int i = 0; i < numThreads ; ++i) {
    		taskVector[i].assign(50, std::deque<Task>() ); // Allocates vector for each thread. "50" is an upper bound limit for
														   // recursion level.
    	}
    	m_lastLevel = 0; // No recursions yet.
    }
    
    // Inserts a task to a thread deque for that task recursion level.
    void insertTask(Task t, int threadID) {
    	if (t.m_rlevel > m_lastLevel)
    		m_lastLevel = t.m_rlevel;
    	taskVector[threadID][t.m_rlevel].push_front(t);
    }
    
    // Return the thread ID assigned to a writing point C(i, j) or -1 if not assigned.  
    int findTaskThread(Task t) {
    	std::tuple<int,int> newTuple (t.m_C_ini_i, t.m_C_ini_j);
    	if (taskMap.find(newTuple) == taskMap.end())
    		return -1;
    	return taskMap[newTuple];		
    }
    
    // Assign a thread ID to a writing point C(i, j).
    void assignThread(Task t, int threadID) {
    	std::tuple<int,int> newTuple (t.m_C_ini_i, t.m_C_ini_j);
    	taskMap[newTuple] = threadID;
    }
    
    // Returns number of tasks generated by threadID with recursion level equal to level.
    int taskCount(int threadID, int level) {
    	return taskVector[threadID][level].size();
    }
    
    // Returns front task of threadID deque with recursion level equal to level.
    Task nextTask(int threadID, int level) {
    	return taskVector[threadID][level].front();
    }
    
    // Removes front task of threadID deque with recursion level equal to level.
    void popTask(int threadID, int level) {
    	taskVector[threadID][level].pop_front();
    }
    
    // Erases taskMap. Called when changing recursion level.
    void cleanMap() {
    	taskMap.clear();
    }
	
};

TaskManager g_taskManager; // Global task manager.

void recursiveMatrixMultiplication(int A_ini_i, int A_ini_j, int A_dim1, int A_dim2, int B_ini_i, int B_ini_j, int B_dim1, int B_dim2, 
								   int rlevel, int id);
void matrixMultiplication(int A_ini_i, int A_ini_j, int A_dim1, int A_dim2, int B_ini_i, int B_ini_j, int B_dim2,
						  int C_ini_i, int C_ini_j, const Array2D<E> &A, const Array2D<E> &B, Array2D<E> &C);


// Lookup table for morton decoding.
static const unsigned short mortonTable256[256] = {
  0x0000, 0x0001, 0x0004, 0x0005, 0x0010, 0x0011, 0x0014, 0x0015, 
  0x0040, 0x0041, 0x0044, 0x0045, 0x0050, 0x0051, 0x0054, 0x0055, 
  0x0100, 0x0101, 0x0104, 0x0105, 0x0110, 0x0111, 0x0114, 0x0115, 
  0x0140, 0x0141, 0x0144, 0x0145, 0x0150, 0x0151, 0x0154, 0x0155, 
  0x0400, 0x0401, 0x0404, 0x0405, 0x0410, 0x0411, 0x0414, 0x0415, 
  0x0440, 0x0441, 0x0444, 0x0445, 0x0450, 0x0451, 0x0454, 0x0455, 
  0x0500, 0x0501, 0x0504, 0x0505, 0x0510, 0x0511, 0x0514, 0x0515, 
  0x0540, 0x0541, 0x0544, 0x0545, 0x0550, 0x0551, 0x0554, 0x0555, 
  0x1000, 0x1001, 0x1004, 0x1005, 0x1010, 0x1011, 0x1014, 0x1015, 
  0x1040, 0x1041, 0x1044, 0x1045, 0x1050, 0x1051, 0x1054, 0x1055, 
  0x1100, 0x1101, 0x1104, 0x1105, 0x1110, 0x1111, 0x1114, 0x1115, 
  0x1140, 0x1141, 0x1144, 0x1145, 0x1150, 0x1151, 0x1154, 0x1155, 
  0x1400, 0x1401, 0x1404, 0x1405, 0x1410, 0x1411, 0x1414, 0x1415, 
  0x1440, 0x1441, 0x1444, 0x1445, 0x1450, 0x1451, 0x1454, 0x1455, 
  0x1500, 0x1501, 0x1504, 0x1505, 0x1510, 0x1511, 0x1514, 0x1515, 
  0x1540, 0x1541, 0x1544, 0x1545, 0x1550, 0x1551, 0x1554, 0x1555, 
  0x4000, 0x4001, 0x4004, 0x4005, 0x4010, 0x4011, 0x4014, 0x4015, 
  0x4040, 0x4041, 0x4044, 0x4045, 0x4050, 0x4051, 0x4054, 0x4055, 
  0x4100, 0x4101, 0x4104, 0x4105, 0x4110, 0x4111, 0x4114, 0x4115, 
  0x4140, 0x4141, 0x4144, 0x4145, 0x4150, 0x4151, 0x4154, 0x4155, 
  0x4400, 0x4401, 0x4404, 0x4405, 0x4410, 0x4411, 0x4414, 0x4415, 
  0x4440, 0x4441, 0x4444, 0x4445, 0x4450, 0x4451, 0x4454, 0x4455, 
  0x4500, 0x4501, 0x4504, 0x4505, 0x4510, 0x4511, 0x4514, 0x4515, 
  0x4540, 0x4541, 0x4544, 0x4545, 0x4550, 0x4551, 0x4554, 0x4555, 
  0x5000, 0x5001, 0x5004, 0x5005, 0x5010, 0x5011, 0x5014, 0x5015, 
  0x5040, 0x5041, 0x5044, 0x5045, 0x5050, 0x5051, 0x5054, 0x5055, 
  0x5100, 0x5101, 0x5104, 0x5105, 0x5110, 0x5111, 0x5114, 0x5115, 
  0x5140, 0x5141, 0x5144, 0x5145, 0x5150, 0x5151, 0x5154, 0x5155, 
  0x5400, 0x5401, 0x5404, 0x5405, 0x5410, 0x5411, 0x5414, 0x5415, 
  0x5440, 0x5441, 0x5444, 0x5445, 0x5450, 0x5451, 0x5454, 0x5455, 
  0x5500, 0x5501, 0x5504, 0x5505, 0x5510, 0x5511, 0x5514, 0x5515, 
  0x5540, 0x5541, 0x5544, 0x5545, 0x5550, 0x5551, 0x5554, 0x5555
};

// Calculates Morton Index(i, j). Matrix where i = line, j = column. i and j are 16-bit size and return is 32-bit size.
unsigned int mortonIndex(unsigned short i, unsigned short j) {
	return mortonTable256[i >> 8]   << 17 | 
           mortonTable256[j >> 8]   << 16 |
           mortonTable256[i & 0xFF] <<  1 | 
           mortonTable256[j & 0xFF];
}

// Returns real index used to access memory.
int realIndex(unsigned int mortonIndex, int i, int j) {
	return mortonIndex * (g_mortonBlockSide * g_mortonBlockSide) + i*g_mortonBlockSide + j;
}


void* initWork(void*);

// Class that handle threads and it's attributes.
class Thread	
{
public:

	int m_identifier;
	int m_type;		   				// Defines this thread doWork function.
	int m_A_ini_i;
	int m_A_ini_j;
	int m_A_dim1;
	int m_A_dim2;
	int m_B_ini_i;
	int m_B_ini_j;
	int m_B_dim1;
	int m_B_dim2;
	int m_rlevel;
	Array2D<E> *m_p_matrix_A;
	Array2D<E> *m_p_matrix_B;
	Array2D<E> *m_p_matrix_C;
	Task m_task;
	std::queue <Task> m_queue;		// Queue of tasks waiting to be processed by this thread.		
	pthread_cond_t  m_taskQueue;	
	pthread_cond_t  m_condWaiting;
	pthread_mutex_t m_mutex;
	pthread_mutex_t m_queueMutex;
	int m_shouldWork;				// Flag to keep working.
	int m_queue_empty;
	int m_waiting;
	int m_started;
	pthread_t m_pt;
	
	
	Thread() { m_started = 0; }

	~Thread() { }

	bool started() const {
		return m_started;
	}
	
	void start(){
		m_started = 1;	
		pthread_create(&m_pt, NULL, initWork, (void*)this);
	}


	void join(){
		if(m_pt != 0 && m_started){
			pthread_join(m_pt,NULL);
			m_started = 0;		
		}
	}

	
	// Called when threads are used to generate tasks.
    void initThread(int A_ini_i, int A_ini_j, int A_dim1, int A_dim2, int B_ini_i, int B_ini_j, int B_dim1, int B_dim2, int rlevel,
    				int identifier) {
    	m_A_ini_i = A_ini_i;
    	m_A_ini_j = A_ini_j;
		m_A_dim1  = A_dim1;
		m_A_dim2  = A_dim2;
		m_B_ini_i = B_ini_i;
		m_B_ini_j = B_ini_j;
		m_B_dim1  = B_dim1;
		m_B_dim2  = B_dim2;
		m_rlevel  = rlevel;
		m_identifier = identifier;
		m_started = 0;
    }
    
    // Called when threads are used to process tasks.
    void initThread_phase2(Array2D<E> *matrix_A, Array2D<E> *matrix_B, Array2D<E> *matrix_C, int id) {
    	m_p_matrix_A = matrix_A;
    	m_p_matrix_B = matrix_B;
    	m_p_matrix_C = matrix_C;
    	m_identifier = id;
    	m_taskQueue = PTHREAD_COND_INITIALIZER;
    	m_condWaiting = PTHREAD_COND_INITIALIZER;
    	m_mutex = PTHREAD_MUTEX_INITIALIZER;
    	m_queueMutex = PTHREAD_MUTEX_INITIALIZER;
    	m_queue_empty = 1;	// Starts with queue empty
    	m_waiting = 0;		// Stats waiting for tasks to process
    	m_started = 0;
    }
    
    // Returns if task queue is empty. Mutex is used to guarantee mutual exclusion. Called by thread and main.
    int queueEmpty() {
    	pthread_mutex_lock(&m_queueMutex);
    	int size = m_queue.size();
    	if (size==1) 
    		m_queue_empty = 1; 				// Task queue is empty.
    	else m_queue_empty = 0;	
    	pthread_mutex_unlock(&m_queueMutex);
    	if (size==0) return 1;
    	return 0;
    }
    
    // Returns next task from queue. Mutual exclusion guaranteed.
    Task queueNext() {
    	pthread_mutex_lock(&m_queueMutex);
    	Task t = m_queue.front();
    	m_queue.pop();
    	pthread_mutex_unlock(&m_queueMutex);
    	return t;
    }
    
    // Inserts new task to this thread task queue.
    void queueInsert(Task t) {
    	pthread_mutex_lock(&m_queueMutex);
    	m_queue.push(t);
    	// If queue was empty we should signal the thread.
    	if (m_queue_empty) {
    		pthread_mutex_unlock(&m_queueMutex);
    		// If thread is waiting for the signal, send the signal.
    		pthread_mutex_lock(&m_mutex);	
    		if (m_waiting) {
    			pthread_cond_signal(&m_taskQueue);
    			pthread_mutex_unlock(&m_mutex);
    		} else { // else wait for thread waiting signal (when the thread finally waits) and send the signal to wake up.
    		    pthread_cond_wait(&m_condWaiting, &m_mutex);
    		    pthread_cond_signal(&m_taskQueue);
    		    pthread_mutex_unlock(&m_mutex);
    		}
    	} else {
    		pthread_mutex_unlock(&m_queueMutex);
    	}
	}
     
    void doWork() {
    	// Generate tasks recursively.
    	if (m_type == 0) {
    		recursiveMatrixMultiplication(m_A_ini_i, m_A_ini_j, m_A_dim1, m_A_dim2, m_B_ini_i, m_B_ini_j, m_B_dim1, m_B_dim2,
    									  m_rlevel, m_identifier);		
    	} else if (m_type == 1) { 
    		// Process tasks from queue.
    		while (m_shouldWork) {
    			pthread_mutex_lock(&m_mutex);
    			m_waiting = 1;
    			pthread_cond_signal(&m_condWaiting);		// Send signal that the thread is waiting.
    			pthread_cond_wait(&m_taskQueue, &m_mutex);	// Wait for avaiable task signal.
    			pthread_mutex_unlock(&m_mutex);
    			if (m_shouldWork==0) break;
    			while (!queueEmpty()) {	 		// Work while there are tasks to process.
    		    	m_task = queueNext();
    				matrixMultiplication(m_task.m_A_ini_i, m_task.m_A_ini_j, m_task.m_A_dim1, m_task.m_A_dim2,m_task.m_B_ini_i, 
										 m_task.m_B_ini_j, m_task.m_B_dim2,m_task.m_C_ini_i, m_task.m_C_ini_j, *m_p_matrix_A, 
										 *m_p_matrix_B, *m_p_matrix_C);
    			}
    		}
    	}
	}
	
};

void* initWork(void* arg){
	Thread* my_thread = (Thread *) arg;
	my_thread->doWork();
	my_thread->m_started = 0;
}

// Matrix Multiplication with loop unrolling x5 and memory blocking.
void matrixMultiplication(int A_ini_i, int A_ini_j, int A_dim1, int A_dim2, int B_ini_i, int B_ini_j, int B_dim2,
						  int C_ini_i, int C_ini_j, const Array2D<E> &A, const Array2D<E> &B, Array2D<E> &C){
	int A_index_i;
	int B_index_j;
	int A_limit_i = A_ini_i + A_dim1; // Index i limit.
	int B_limit_j = B_ini_j + B_dim2; // Index j limit.
	int k;
	int limit_k = A_dim2;
	
	// Computes Z-order indexes.
	int A_morton_i = A_ini_i/g_mortonBlockSide;
	int A_morton_j = A_ini_j/g_mortonBlockSide;
	int B_morton_i = B_ini_i/g_mortonBlockSide;
	int B_morton_j = B_ini_j/g_mortonBlockSide;
	int C_morton_i = C_ini_i/g_mortonBlockSide;
	int C_morton_j = C_ini_j/g_mortonBlockSide;
	
	// Used when looping through more than one morton block.
	int A_morton_j_aux = A_morton_j;
	int B_morton_i_aux = B_morton_i;
	int B_morton_j_aux = B_morton_j;
	int C_morton_j_aux = C_morton_j;
	
	// Real indexes used in memory.
	int A_mortonInd = mortonIndex(A_morton_i, A_morton_j);
	int A_realInd = realIndex(A_mortonInd, 0, 0);
	int B_mortonInd = mortonIndex(B_morton_j, B_morton_i);
	int B_realInd = realIndex(B_mortonInd, 0, 0);
	int C_mortonInd = mortonIndex(C_morton_i, C_morton_j);
	int C_realInd = realIndex(C_mortonInd, 0, 0);
	
    int aux_i, aux_j, aux_k;
    int aux_limit_i; 				// Block index limit i
    int aux_limit_j; 				// Block index limit j
    int aux_limit_k; 				// Block index limit k
    int unroll_factor = 5;
    int unroll_limit; 				// Loop unroll index limit
    E acc0, acc1, acc2, acc3, acc4; // Accumulators, eliminate data dependencies
    
    for (A_index_i = A_ini_i; A_index_i < A_limit_i; A_index_i += g_cacheBlockSize) {
        // Blocking index i limit
        aux_limit_i = min((A_index_i+g_cacheBlockSize), A_limit_i);
 		
 		// If acessing new morton block, update morton indexes.
        if ((A_index_i-A_ini_i)%g_mortonBlockSide==0 && (A_index_i-A_ini_i)!=0) {
	        A_morton_i++;
	        C_morton_i++;
		}
		
		// Going back to initial morton block for collumn
        B_morton_j = B_morton_j_aux;
        C_morton_j = C_morton_j_aux;
    	for (B_index_j = B_ini_j; B_index_j < B_limit_j; B_index_j += g_cacheBlockSize) {
    	    // Blocking index j limit
    	    aux_limit_j = min((B_index_j+g_cacheBlockSize), B_limit_j);
    		
    		// If acessing new morton block, update morton indexes.
    	    if ((B_index_j-B_ini_j)%g_mortonBlockSide==0 && (B_index_j-B_ini_j)!=0) {
	        	B_morton_j++;
	        	C_morton_j++;
			}	
			
			// Calculates morton block and real index to store on matrix C.
			C_mortonInd = mortonIndex(C_morton_i, C_morton_j);
			C_realInd = realIndex(C_mortonInd, 0, 0);
			
    	    A_morton_j = A_morton_j_aux;
    	    B_morton_i = B_morton_i_aux;
    		for (k = 0; k < limit_k; k += g_cacheBlockSize) {
    		    // Blocking index k limit
    		    aux_limit_k = min((k+g_cacheBlockSize), limit_k);
				
				// If looping through new morton block, update indexes.
    		    if (k%g_mortonBlockSide==0 && k!=0) {
	       	 		A_morton_j++;
	       	 		B_morton_i++;
				}		
				
				// Calculates morton index and real index for matrices A and B.
    		    A_mortonInd = mortonIndex(A_morton_i, A_morton_j);
				A_realInd = realIndex(A_mortonInd, 0, 0);
				B_mortonInd = mortonIndex(B_morton_i, B_morton_j);
				B_realInd = realIndex(B_mortonInd, 0, 0);
				
                unroll_limit = aux_limit_k - (unroll_factor-1); // Unrolling by factor of 5
                
              	for(aux_i = A_index_i; aux_i < aux_limit_i; ++aux_i) {
                	for(aux_j = B_index_j; aux_j < aux_limit_j; ++aux_j) {
                    	acc0 = 0; acc1 = 0; acc2 = 0; acc3 = 0; acc4 = 0;
                    	
                    	// Update real indexes inside morton block.
						int finalIndexA = A_realInd + ((aux_i-A_ini_i)%g_mortonBlockSide)*g_mortonBlockSide;
						int finalIndexB = B_realInd + ((aux_j-B_ini_j)%g_mortonBlockSide)*g_mortonBlockSide;

						// Unrolling for k loop
                    	for(aux_k = k; aux_k < unroll_limit; aux_k+=unroll_factor) {
                    		// Index k inside the morton block
                    	    int mod_aux_k = aux_k%g_mortonBlockSide;
                        	acc0 += A (finalIndexA+mod_aux_k) * B (finalIndexB+mod_aux_k);
                        	acc1 += A (finalIndexA+mod_aux_k+1) * B (finalIndexB+mod_aux_k+1);
                        	acc2 += A (finalIndexA+mod_aux_k+2) * B (finalIndexB+mod_aux_k+2);
                        	acc3 += A (finalIndexA+mod_aux_k+3) * B (finalIndexB+mod_aux_k+3);
                        	acc4 += A (finalIndexA+mod_aux_k+4) * B (finalIndexB+mod_aux_k+4);
                        } 
                        
                        // Update real indexes inside morton block.
                        int finalIndexC = C_realInd + ((aux_i-A_ini_i) % g_mortonBlockSide) * g_mortonBlockSide + 
                        		           (aux_j-B_ini_j) % g_mortonBlockSide;
                        		           
                        // Gather possible uncounted elements
                        for (; aux_k < aux_limit_k; ++aux_k){
                        	// Index k inside the morton block
                        	int mod_aux_k = aux_k % g_mortonBlockSide;
                        	C(finalIndexC) += A (finalIndexA+mod_aux_k) * B (finalIndexB+mod_aux_k); 	
						}
						
                        // Sum up everything
                        C(finalIndexC) += acc0 + acc1 + acc2 + acc3 + acc4; 

                	}  
            	}   
			}
		}
	}
    return;
}

// Create task and send it to task manager.
void createTask(int A_ini_i, int A_ini_j, int A_dim1, int A_dim2, int B_ini_i, int B_ini_j, int B_dim2, int C_ini_i, int C_ini_j, 				    int rlevel, int id) {
    Task t(A_ini_i, A_ini_j, A_dim1, A_dim2, B_ini_i, B_ini_j, B_dim2, C_ini_i, C_ini_j, rlevel);
	g_taskManager.insertTask(t, id);
}

// Recursive Matrix Multiplication called by threads to generate tasks.
void recursiveMatrixMultiplication(int A_ini_i, int A_ini_j, int A_dim1, int A_dim2, int B_ini_i, int B_ini_j, int B_dim1, int B_dim2, 
								   int rlevel, int id) {
	
	// Split matrix A and matrix B in four sub matrices. Split them in blocks of Z-order.
	
	// Calculates how many blocks on each dimension and divide by two.							   	
	int A_dim1_blocks = (ceil)((double)A_dim1/g_mortonBlockSide);
	int A_dim1_left = (A_dim1_blocks/2)*g_mortonBlockSide;
	if (A_dim1_blocks%2!=0)
		A_dim1_left += g_mortonBlockSide;
		
	int A_dim2_blocks = (ceil)((double)A_dim2/g_mortonBlockSide);
	int A_dim2_left = (A_dim2_blocks/2)*g_mortonBlockSide;
	if (A_dim2_blocks%2!=0)
		A_dim2_left += g_mortonBlockSide;	
	
	int B_dim1_blocks = (ceil)((double)B_dim1/g_mortonBlockSide);
	int B_dim1_left = (B_dim1_blocks/2)*g_mortonBlockSide;
	if (B_dim1_blocks%2!=0)
		B_dim1_left += g_mortonBlockSide;
		
	int B_dim2_blocks = (ceil)((double)B_dim2/g_mortonBlockSide);
	int B_dim2_left = (B_dim2_blocks/2)*g_mortonBlockSide;
	if (B_dim2_blocks%2!=0)
		B_dim2_left += g_mortonBlockSide;
	
	// Calculates all 8 submatrices (A and B)
	
	int A11_dim1 = A_dim1_left;
	int A11_dim2 = A_dim2_left;
	int A11_ini_i = A_ini_i;
	int A11_ini_j = A_ini_j;
	
	int A21_dim1 = A_dim1 - A11_dim1;
	int A21_dim2 = A_dim2_left;
	int A21_ini_i = A_ini_i + A11_dim1;
	int A21_ini_j = A_ini_j;
	
	int A12_dim1 = A_dim1_left;
	int A12_dim2 = A_dim2 - A11_dim2;
	int A12_ini_i = A_ini_i;
	int A12_ini_j = A_ini_j + A11_dim2;
	
	int A22_dim1 = A_dim1 - A11_dim1;
	int A22_dim2 = A_dim2 - A11_dim2;	
	int A22_ini_i = A_ini_i + A11_dim1;
	int A22_ini_j = A_ini_j + A11_dim2;
	
	int B11_dim1 = B_dim1_left;
	int B11_dim2 = B_dim2_left;
	int B11_ini_i = B_ini_i;
	int B11_ini_j = B_ini_j;
	
	int B21_dim1 = B_dim1 - B11_dim1;
	int B21_dim2 = B_dim2_left;	
	int B21_ini_i = B_ini_i + B11_dim1;
	int B21_ini_j = B_ini_j;
	
	int B12_dim1 = B_dim1_left;
	int B12_dim2 = B_dim2 - B11_dim2;
	int B12_ini_i = B_ini_i;
	int B12_ini_j = B_ini_j + B11_dim2;
	
	int B22_dim1 = B_dim1 - B11_dim1;
	int B22_dim2 = B_dim2 - B11_dim2;	
	int B22_ini_i = B_ini_i + B11_dim1;
	int B22_ini_j = B_ini_j + B11_dim2;		
	
	// C11
	// ----A11*B11 + A12*B21----
	
	// A11*B11
	if ( (A11_dim1 <= g_mortonBlockSide || A11_dim2 <= g_mortonBlockSide || B11_dim2 <= g_mortonBlockSide) || 
		 (A11_dim1 == 1 || A11_dim2 == 1 || B11_dim2 == 1) ) {
		// If reached block size can't split anymore. Create task.
		createTask(A11_ini_i, A11_ini_j, A11_dim1, A11_dim2, B11_ini_i, B11_ini_j, B11_dim2, A11_ini_i,B11_ini_j, rlevel+1, 
				   id);		 
	} else { 
		// Can split into new submatrices, call recursion.
		recursiveMatrixMultiplication(A11_ini_i, A11_ini_j, A11_dim1, A11_dim2, B11_ini_i, B11_ini_j, B11_dim1, B11_dim2, rlevel+1, 
									  id);
	} 
	// A12*B21
	if ( (A12_dim1 <= g_mortonBlockSide || A12_dim2 <= g_mortonBlockSide || B21_dim2 <= g_mortonBlockSide) || 
		 (A12_dim1 == 1 || A12_dim2 == 1 || B21_dim2 == 1) ) {
		createTask(A12_ini_i, A12_ini_j, A12_dim1, A12_dim2, B21_ini_i, B21_ini_j, B21_dim2, A11_ini_i,B11_ini_j, rlevel+1, 
				   id);		 
	} else { 
		recursiveMatrixMultiplication(A12_ini_i, A12_ini_j, A12_dim1, A12_dim2, B21_ini_i, B21_ini_j, B21_dim1, B21_dim2, rlevel+1, 
									  id);
	} 
	
	// C12
	// ----A11*B12 + A12*B22----
	
	// A11*B12
	if ( (A11_dim1 <= g_mortonBlockSide || A11_dim2 <= g_mortonBlockSide || B12_dim2 <= g_mortonBlockSide) || 
		 (A11_dim1 == 1 || A11_dim2 == 1 || B12_dim2 == 1) ) {
		createTask(A11_ini_i, A11_ini_j, A11_dim1, A11_dim2, B12_ini_i, B12_ini_j, B12_dim2, A12_ini_i,B12_ini_j, rlevel+1, 
				   id);		 
	} else { 
		recursiveMatrixMultiplication(A11_ini_i, A11_ini_j, A11_dim1, A11_dim2, B12_ini_i, B12_ini_j, B12_dim1, B12_dim2, rlevel+1, 
									  id);
	} 
	
	// A12*B22
	if ( (A12_dim1 <= g_mortonBlockSide || A12_dim2 <= g_mortonBlockSide || B22_dim2 <= g_mortonBlockSide) || 
		 (A12_dim1 == 1 || A12_dim2 == 1 || B22_dim2 == 1) ) {
		createTask(A12_ini_i, A12_ini_j, A12_dim1, A12_dim2, B22_ini_i, B22_ini_j, B22_dim2, A12_ini_i,B12_ini_j, rlevel+1, 
				   id);		 
	} else { 
		recursiveMatrixMultiplication(A12_ini_i, A12_ini_j, A12_dim1, A12_dim2, B22_ini_i, B22_ini_j, B22_dim1, B22_dim2, rlevel+1, 
									  id);
	} 
	
	// C21
	// ----A21*B11 + A22*B21----
	
	// A21*B11
	if ( (A21_dim1 <= g_mortonBlockSide || A21_dim2 <= g_mortonBlockSide || B11_dim2 <= g_mortonBlockSide) || 
		 (A21_dim1 == 1 || A21_dim2 == 1 || B11_dim2 == 1) ) {
		createTask(A21_ini_i, A21_ini_j, A21_dim1, A21_dim2, B11_ini_i, B11_ini_j, B11_dim2, A21_ini_i,B21_ini_j, rlevel+1, 
				   id);		 
	} else { 
		recursiveMatrixMultiplication(A21_ini_i, A21_ini_j, A21_dim1, A21_dim2, B11_ini_i, B11_ini_j, B11_dim1, B11_dim2, rlevel+1, 
									  id);
	} 

	// A22*B21
	if ( (A22_dim1 <= g_mortonBlockSide || A22_dim2 <= g_mortonBlockSide || B21_dim2 <= g_mortonBlockSide) || 
		 (A22_dim1 == 1 || A22_dim2 == 1 || B21_dim2 == 1) ) {
		createTask(A22_ini_i, A22_ini_j, A22_dim1, A22_dim2, B21_ini_i, B21_ini_j, B21_dim2, A21_ini_i,B21_ini_j, rlevel+1, 
				   id);		 
	} else { 
		recursiveMatrixMultiplication(A22_ini_i, A22_ini_j, A22_dim1, A22_dim2, B21_ini_i, B21_ini_j, B21_dim1, B21_dim2, rlevel+1, 
									  id);
	} 
	
	// C22
	// ----A21*B12 + A22*B22----	
	
	// A21*B12
	if ( (A21_dim1 <= g_mortonBlockSide || A21_dim2 <= g_mortonBlockSide || B12_dim2 <= g_mortonBlockSide) || 
		 (A21_dim1 == 1 || A21_dim2 == 1 || B12_dim2 == 1) ) {
		createTask(A21_ini_i, A21_ini_j, A21_dim1, A21_dim2, B12_ini_i, B12_ini_j, B12_dim2, A22_ini_i,B22_ini_j, rlevel+1, 
				   id);		 
	} else { 
		recursiveMatrixMultiplication(A21_ini_i, A21_ini_j, A21_dim1, A21_dim2, B12_ini_i, B12_ini_j, B12_dim1, B12_dim2, rlevel+1, 
									  id);
	} 
	
	// A22*B22
	if ( (A22_dim1 <= g_mortonBlockSide || A22_dim2 <= g_mortonBlockSide || B22_dim2 <= g_mortonBlockSide) || 
		 (A22_dim1 == 1 || A22_dim2 == 1 || B22_dim2 == 1) ) {
		createTask(A22_ini_i, A22_ini_j, A22_dim1, A22_dim2, B22_ini_i, B22_ini_j, B22_dim2, A22_ini_i,B22_ini_j, rlevel+1, 
				   id);		 
	} else { 
		recursiveMatrixMultiplication(A22_ini_i, A22_ini_j, A22_dim1, A22_dim2, B22_ini_i, B22_ini_j, B22_dim1, B22_dim2, rlevel+1, 
									  id);
	} 
				   
								   
}

// Function splits matrix A and B and assign each of 8 submatrices to avaiable threads to recursively generate tasks.
// Them manage those threads to process tasks.
void taskScheduler(int A_ini_i, int A_ini_j, int A_dim1, int A_dim2, int B_ini_i, int B_ini_j, int B_dim1, int B_dim2, 
				   Array2D<E> *matrix_A, Array2D<E> *matrix_B, Array2D<E> *matrix_C, Thread * thread, int numThreads) {
	
	// Split matrix A and matrix B in four sub matrices. Split them in blocks of Z-order.
	
	// Calculates how many blocks on each dimension and divide by two.	
	int A_dim1_blocks = (ceil)((double)A_dim1/g_mortonBlockSide);
	int A_dim1_left = (A_dim1_blocks/2)*g_mortonBlockSide;
	if (A_dim1_blocks%2!=0)
		A_dim1_left += g_mortonBlockSide;
		
	int A_dim2_blocks = (ceil)((double)A_dim2/g_mortonBlockSide);
	int A_dim2_left = (A_dim2_blocks/2)*g_mortonBlockSide;
	if (A_dim2_blocks%2!=0)
		A_dim2_left += g_mortonBlockSide;	
	
	int B_dim1_blocks = (ceil)((double)B_dim1/g_mortonBlockSide);
	int B_dim1_left = (B_dim1_blocks/2)*g_mortonBlockSide;
	if (B_dim1_blocks%2!=0)
		B_dim1_left += g_mortonBlockSide;
		
	int B_dim2_blocks = (ceil)((double)B_dim2/g_mortonBlockSide);
	int B_dim2_left = (B_dim2_blocks/2)*g_mortonBlockSide;
	if (B_dim2_blocks%2!=0)
		B_dim2_left += g_mortonBlockSide;
	
	// Quadrants order
    // C11 - 0 | 1 - C12
	// 		 -----
	// C21 - 2 | 3 - C22
	
	// Calculates all 8 submatrices (A and B)
	
	// A
	int A11_dim1 = A_dim1_left;
	int A11_dim2 = A_dim2_left;
	int A11_ini_i = A_ini_i;
	int A11_ini_j = A_ini_j;
	
	int A21_dim1 = A_dim1 - A11_dim1;
	int A21_dim2 = A_dim2_left;
	int A21_ini_i = A_ini_i + A11_dim1;
	int A21_ini_j = A_ini_j;
	
	int A12_dim1 = A_dim1_left;
	int A12_dim2 = A_dim2 - A11_dim2;
	int A12_ini_i = A_ini_i;
	int A12_ini_j = A_ini_j + A11_dim2;
	
	int A22_dim1 = A_dim1 - A11_dim1;
	int A22_dim2 = A_dim2 - A11_dim2;	
	int A22_ini_i = A_ini_i + A11_dim1;
	int A22_ini_j = A_ini_j + A11_dim2;
	
	// B
	int B11_dim1 = B_dim1_left;
	int B11_dim2 = B_dim2_left;
	int B11_ini_i = B_ini_i;
	int B11_ini_j = B_ini_j;
	
	int B21_dim1 = B_dim1 - B11_dim1;
	int B21_dim2 = B_dim2_left;	
	int B21_ini_i = B_ini_i + B11_dim1;
	int B21_ini_j = B_ini_j;
	
	int B12_dim1 = B_dim1_left;
	int B12_dim2 = B_dim2 - B11_dim2;
	int B12_ini_i = B_ini_i;
	int B12_ini_j = B_ini_j + B11_dim2;
	
	int B22_dim1 = B_dim1 - B11_dim1;
	int B22_dim2 = B_dim2 - B11_dim2;	
	int B22_ini_i = B_ini_i + B11_dim1;
	int B22_ini_j = B_ini_j + B11_dim2;			
		
	int nextThread = 0;
	int task_gen_maxThreads = min(8, numThreads); // Task generation limited to 8 threads in parallel. One for each submatrix.
	int rlevel = 0;								  // Recursion level.
	
	// C11
	// ----A11*B11 + A12*B21----
	
	// A11*B11
	// Threads being called sequentially. If this one is working others should be working also.
	if (thread[nextThread].started()) {
		thread[nextThread].join();
	}

	if ( (A11_dim1 <= g_mortonBlockSide || A11_dim2 <= g_mortonBlockSide || B11_dim2 <= g_mortonBlockSide) || 
 		 (A11_dim1 == 1 || A11_dim2 == 1 || B11_dim2 == 1) ) {
		// If reached block size can't split anymore. Create task.
		createTask(A11_ini_i, A11_ini_j, A11_dim1, A11_dim2, B11_ini_i, B11_ini_j, B11_dim2, A11_ini_i,B11_ini_j, rlevel, 
		   		   nextThread);		 
	} else {
		// Can split into submatrices. Prepare thread and start executing it.
		thread[nextThread].initThread(A11_ini_i, A11_ini_j, A11_dim1, A11_dim2, B11_ini_i, B11_ini_j, B11_dim1, B11_dim2, rlevel, 
			   						   nextThread);
		thread[nextThread].m_type = 0; // Task generation.
		thread[nextThread].start();	   // Will call recursiveMatrixMultiplication function to generate tasks.  
		nextThread = (nextThread+1) % task_gen_maxThreads; 	
	}
	
	// A12*B21
	if (thread[nextThread].started()) {
		thread[nextThread].join();
	}
	
	if ( (A12_dim1 <= g_mortonBlockSide || A12_dim2 <= g_mortonBlockSide || B21_dim2 <= g_mortonBlockSide) || 
		 (A12_dim1 == 1 || A12_dim2 == 1 || B21_dim2 == 1) ) {
		createTask(A12_ini_i, A12_ini_j, A12_dim1, A12_dim2, B21_ini_i, B21_ini_j, B21_dim2, A11_ini_i,B11_ini_j, rlevel, 
				   nextThread);		 
	} else { 
		thread[nextThread].initThread(A12_ini_i, A12_ini_j, A12_dim1, A12_dim2, B21_ini_i, B21_ini_j, B21_dim1, B21_dim2, rlevel, 
									  nextThread);
		thread[nextThread].m_type = 0;
		thread[nextThread].start();
		nextThread = (nextThread+1) % task_gen_maxThreads;							  	
	}

	// C12
	// ----A11*B12 + A12*B22----
	
	// A11*B12
	if (thread[nextThread].started()) {
		thread[nextThread].join();
	}
	
	if ( (A11_dim1 <= g_mortonBlockSide || A11_dim2 <= g_mortonBlockSide || B12_dim2 <= g_mortonBlockSide) || 
		 (A11_dim1 == 1 || A11_dim2 == 1 || B12_dim2 == 1) ) {
		createTask(A11_ini_i, A11_ini_j, A11_dim1, A11_dim2, B12_ini_i, B12_ini_j, B12_dim2, A12_ini_i,B12_ini_j, rlevel, 
			       nextThread);		 
	} else { 
		thread[nextThread].initThread(A11_ini_i, A11_ini_j, A11_dim1, A11_dim2, B12_ini_i, B12_ini_j, B12_dim1, B12_dim2, rlevel,
									  nextThread);
		thread[nextThread].m_type = 0;
		thread[nextThread].start();		
		nextThread = (nextThread+1) % task_gen_maxThreads;					  
	} 

	// A12*B22
	if (thread[nextThread].started()) {
		thread[nextThread].join();
	}
	
	if ( (A12_dim1 <= g_mortonBlockSide || A12_dim2 <= g_mortonBlockSide || B22_dim2 <= g_mortonBlockSide) || 
		 (A12_dim1 == 1 || A12_dim2 == 1 || B22_dim2 == 1) ) {
		createTask(A12_ini_i, A12_ini_j, A12_dim1, A12_dim2, B22_ini_i, B22_ini_j, B22_dim2, A12_ini_i,B12_ini_j, rlevel, 
				   nextThread);		 
	} else { 
		thread[nextThread].initThread(A12_ini_i, A12_ini_j, A12_dim1, A12_dim2, B22_ini_i, B22_ini_j, B22_dim1, B22_dim2, rlevel, 
									  nextThread);
		thread[nextThread].m_type = 0;
		thread[nextThread].start();		
		nextThread = (nextThread+1) % task_gen_maxThreads;				  
	} 

	// C21
	// ----A21*B11 + A22*B21----
	
	// A21*B11
	if (thread[nextThread].started()) {
		thread[nextThread].join();
	}
	
	if ( (A21_dim1 <= g_mortonBlockSide || A21_dim2 <= g_mortonBlockSide || B11_dim2 <= g_mortonBlockSide) || 
		 (A21_dim1 == 1 || A21_dim2 == 1 || B11_dim2 == 1) ) {
		createTask(A21_ini_i, A21_ini_j, A21_dim1, A21_dim2, B11_ini_i, B11_ini_j, B11_dim2, A21_ini_i,B21_ini_j, rlevel, 
				   nextThread);		 
	} else { 
		thread[nextThread].initThread(A21_ini_i, A21_ini_j, A21_dim1, A21_dim2, B11_ini_i, B11_ini_j, B11_dim1, B11_dim2, rlevel, 
								      nextThread);
		thread[nextThread].m_type = 0;
		thread[nextThread].start();	
		nextThread = (nextThread+1) % task_gen_maxThreads;			    
	} 

	// A22*B21
	if (thread[nextThread].started()) {
		thread[nextThread].join();
	}
	
	if ( (A22_dim1 <= g_mortonBlockSide || A22_dim2 <= g_mortonBlockSide || B21_dim2 <= g_mortonBlockSide) || 
		 (A22_dim1 == 1 || A22_dim2 == 1 || B21_dim2 == 1) ) {
		createTask(A22_ini_i, A22_ini_j, A22_dim1, A22_dim2, B21_ini_i, B21_ini_j, B21_dim2, A21_ini_i,B21_ini_j, rlevel, 
				   nextThread);		 
	} else { 
		thread[nextThread].initThread(A22_ini_i, A22_ini_j, A22_dim1, A22_dim2, B21_ini_i, B21_ini_j, B21_dim1, B21_dim2, rlevel, 
									  nextThread);
		thread[nextThread].m_type = 0;
		thread[nextThread].start();	
		nextThread = (nextThread+1) % task_gen_maxThreads;						  
	} 

	// C22
	// ----A21*B12 + A22*B22----	
	
	// A21*B12
	if (thread[nextThread].started()) {
		thread[nextThread].join();
	}
	
	if ( (A21_dim1 <= g_mortonBlockSide || A21_dim2 <= g_mortonBlockSide || B12_dim2 <= g_mortonBlockSide) || 
		 (A21_dim1 == 1 || A21_dim2 == 1 || B12_dim2 == 1) ) {
		createTask(A21_ini_i, A21_ini_j, A21_dim1, A21_dim2, B12_ini_i, B12_ini_j, B12_dim2, A22_ini_i,B22_ini_j, rlevel, 
				   nextThread);		 
	} else { 
		thread[nextThread].initThread(A21_ini_i, A21_ini_j, A21_dim1, A21_dim2, B12_ini_i, B12_ini_j, B12_dim1, B12_dim2, rlevel, 
									  nextThread);
		thread[nextThread].m_type = 0;
		thread[nextThread].start();		
		nextThread = (nextThread+1) % task_gen_maxThreads;					  
	} 

	// A22*B22
	if (thread[nextThread].started()) {
		thread[nextThread].join();
	}
	
	if ( (A22_dim1 <= g_mortonBlockSide || A22_dim2 <= g_mortonBlockSide || B22_dim2 <= g_mortonBlockSide) || 
		 (A22_dim1 == 1 || A22_dim2 == 1 || B22_dim2 == 1) ) {
		createTask(A22_ini_i, A22_ini_j, A22_dim1, A22_dim2, B22_ini_i, B22_ini_j, B22_dim2, A22_ini_i,B22_ini_j, rlevel, 
				   nextThread);		 
	} else { 
		thread[nextThread].initThread(A22_ini_i, A22_ini_j, A22_dim1, A22_dim2, B22_ini_i, B22_ini_j, B22_dim1, B22_dim2, rlevel, 
									   nextThread);
		thread[nextThread].m_type = 0;
		thread[nextThread].start();							   
	} 

	// Synchronize all threads. Wait for all of them to finish computing tasks.
	for (int i = 0; i < task_gen_maxThreads; ++i) {
		if (thread[i].started())
			thread[i].join();
	}
	
	nextThread = 0;
	Task nextTask;
	int currentLevel = g_taskManager.m_lastLevel;	// Must start computing from deepest recursion level.
	int thread_assigned;

	for (int i = 0; i < numThreads; ++i){
		// Start threads to work for processing tasks.
		thread[i].initThread_phase2(matrix_A, matrix_B, matrix_C, i);
		thread[i].m_type = 1; 	
		thread[i].m_shouldWork = 1;	
		thread[i].start();
	}
	
	// Loop through all recursion levels.
	while(currentLevel >= 0) {
		// Check for deques from each generating thread if there are tasks with current recursion level.
		for (int i = 0; i < task_gen_maxThreads; ++i) {
			// While there are tasks with current recursion level.
			while (g_taskManager.taskCount(i, currentLevel) != 0 ) {
				// Get Task.
				nextTask = g_taskManager.nextTask(i, currentLevel);
				// Remove Task from deque.
				g_taskManager.popTask(i, currentLevel);
				// Find thread ID assigned to this task writing point C(i, j).
				thread_assigned = g_taskManager.findTaskThread(nextTask);
				// If no thread assigned to it.
				if (thread_assigned == -1){
					// Assign to a new one.
					g_taskManager.assignThread(nextTask, nextThread);
					thread_assigned = nextThread;
					nextThread = (nextThread + 1) % numThreads;
				}	
				// Insert task to working thread assigned to this task writing point.
				thread[thread_assigned].queueInsert(nextTask);
			}
		}
		// Ended a recursion level. Synchronize. Wait for all threads.
		for (int i = 0; i < numThreads; ++i) {	
			 // Thread is working, wait for it to finish working.
			 if (thread[i].m_waiting==0)
				 pthread_cond_wait(&thread[i].m_condWaiting, &thread[i].m_mutex);
			 // If thread is waiting and there is task to be executed, wake up thread and wait it finish.
			 while (!thread[i].queueEmpty()) {
				pthread_cond_signal(&thread[i].m_taskQueue);	
				if (thread[i].m_waiting==0)
					pthread_cond_wait(&thread[i].m_condWaiting, &thread[i].m_mutex);
			 }
		}
		// Decrease recursion level.
		currentLevel--;
		
		// New recursion level, erase previous map.
		g_taskManager.cleanMap();
	}

	// Last synchronize pass. Signal all threads to stop working.
	for (int i = 0; i < numThreads; ++i) {	
		if (thread[i].m_waiting==0)
			pthread_cond_wait(&thread[i].m_condWaiting, &thread[i].m_mutex);
		while (!thread[i].queueEmpty()) {
			pthread_cond_signal(&thread[i].m_taskQueue);	
			if (thread[i].m_waiting==0)
				pthread_cond_wait(&thread[i].m_condWaiting, &thread[i].m_mutex);
		}
		// Set flag that thread should stop working.
		thread[i].m_shouldWork = 0;
		pthread_cond_signal(&thread[i].m_taskQueue);
		if (thread[i].started())
			thread[i].join();
	}
}

// Read matrices A and B from input text
int readInput(Array2D<E> &matrix_A, Array2D<E> &matrix_B, int m, int k, int n) {
	ifstream file_matrix_A;
	ifstream file_matrix_B;
	file_matrix_A.open ("matrix_A.txt", std::ifstream::in); //File to read matrix A
	file_matrix_B.open ("matrix_B.txt", std::ifstream::in); //File to read matrix B
	
	int dim1 = m;
	int dim2 = k;
	
	int mortonInd;
	int morton_i = 0;
	int morton_j = 0;
	int realInd;

	if (file_matrix_A.is_open()) {
	
		//Matrix A is stored with Z-order Layout.
		for (int i = 0; i < dim1; i+=g_mortonBlockSide, ++morton_i) {
	
			for (int l = 0; l < g_mortonBlockSide && (l+i) < dim1; ++l) {
		
				for (int j = 0, morton_j = 0; j < dim2; j+=g_mortonBlockSide, ++morton_j) {
					
					// Computes morton index.
					mortonInd = mortonIndex(morton_i, morton_j);
	
					for (int k = 0; k < g_mortonBlockSide && (k+j) < dim2; ++k) {
						// Real index used by memory.
						realInd = realIndex(mortonInd, l, k);
						file_matrix_A >> matrix_A(realInd); 	
					}
				}
			}
		}
	} else return 0;
	
	file_matrix_A.close();
	
	if (file_matrix_B.is_open()) {
		dim1 = k;
		dim2 = n;
		morton_i = 0;
		
		// Matrix B is stored with Z-order Layout and Column major order
		for (int i = 0; i < dim1; i+=g_mortonBlockSide, ++morton_i) {
	
			for (int l = 0; l < g_mortonBlockSide && (l+i) < dim1; ++l) {
	
				for (int j = 0, morton_j = 0; j < dim2; j+=g_mortonBlockSide, ++morton_j) {
					
					// Computes morton index.
					mortonInd = mortonIndex(morton_i, morton_j);
	
					for (int k = 0; k < g_mortonBlockSide && (k+j) < dim2; ++k) {
						// Real index used by memory.
						realInd = realIndex(mortonInd, k, l);
						file_matrix_B >> matrix_B(realInd); 	
					}
				}
			}
		}
	} else return 0;
	
	file_matrix_B.close();
	
	return 1;
}

// Size allocated must be a power of two because of Z-order memory layout.
void allocateMatrices(Array2D<E> &matrix_A, Array2D<E> &matrix_B, Array2D<E> &matrix_C, int m, int k, int n) {
	int m_size = ceil((double)m/g_mortonBlockSide)*g_mortonBlockSide;
	int k_size = ceil((double)k/g_mortonBlockSide)*g_mortonBlockSide;
	int n_size = ceil((double)n/g_mortonBlockSide)*g_mortonBlockSide;
	
	// Power of two sizes
	m_size = pow(2, ceil(log2 ((double) m_size) ));
	k_size = pow(2, ceil(log2 ((double) k_size) ));
	n_size = pow(2, ceil(log2 ((double) n_size) ));
	
	// Square matrices
	int A_side = max(m_size, k_size);
	int B_side = max(k_size, n_size);
	int C_side = max(m_size, n_size);
	
	// Fill matrices with 0
	matrix_A.build(A_side, A_side, 0, 0);
	matrix_B.build(B_side, B_side, 0, 0);
	matrix_C.build(C_side, C_side, 0, 0);
}

int readDimensions(int* m, int* k, int* n, int* cacheBlockSize, int* dataType) {
	ifstream file_input;
	file_input.open ("matrix_dim.txt", std::ifstream::in); //File to read matrices dimensions
	if (file_input.is_open()) {
		file_input >> *m >> *k >> *n >> *cacheBlockSize >> *dataType;
		file_input.close();
	} else return 0;
	return 1;
}

void writeOutput(Array2D<E> &matrix, int m, int n){
    ofstream file_out;
    
    #ifdef FORCE_SINGLE_CORE
		file_out.open("recursiveMult_matrix_C.txt");
	#else
		file_out.open("recursiveMultParallel_matrix_C.txt");	
	#endif
		
    file_out << std::fixed;
    
	int dim1 = m;
	int dim2 = n;
    
    int morton_i = 0;
	int morton_j = 0;
    int mortonInd;
    int realInd;
    if (file_out.is_open()) {
		for (int i = 0; i < dim1; i+=g_mortonBlockSide, ++morton_i) {

			for (int l = 0; l < g_mortonBlockSide && (l+i) < dim1; ++l) {

				for (int j = 0, morton_j = 0; j < dim2; j+=g_mortonBlockSide, ++morton_j) {
					
					// Computes morton index.
					mortonInd = mortonIndex(morton_i, morton_j);

					for (int k = 0; k < g_mortonBlockSide && (k+j) < dim2; ++k) {
						// Real index used by memory.
						realInd = realIndex(mortonInd, l, k);	
						file_out << std::setprecision(g_outputPrecision) << std::setw(g_outputWidth) << matrix(realInd) << " ";
					}
				}
				file_out << endl;
			}
		}
	}	
}

void getTiming(double* timing, struct timeval* time_start, struct timeval* time_end) {
	*timing = 0;
	
	*timing = (double)(time_end->tv_sec- time_start->tv_sec)*1000;
	*timing += (double)(time_end->tv_usec - time_start->tv_usec)/1000;
	
	return;				   
}

void writeReadTime(double time){
	ofstream file_out;
	#ifdef FORCE_SINGLE_CORE
		file_out.open("recursiveMult_read_time.txt");
	#else
		file_out.open("recursiveMultParallel_read_time.txt");
	#endif
	if (file_out.is_open()) {
		file_out << time;
		file_out.close();
	}	
}

void writeWriteTime(double time){
	ofstream file_out;
	#ifdef FORCE_SINGLE_CORE
		file_out.open("recursiveMult_write_time.txt");
	#else
		file_out.open("recursiveMultParallel_write_time.txt");
	#endif
	if (file_out.is_open()) {
		file_out << time;
		file_out.close();
	}	
}

void writeProcessTime(double time){
	ofstream file_out;
	#ifdef FORCE_SINGLE_CORE
		file_out.open("recursiveMult_process_time.txt");
	#else
		file_out.open("recursiveMultParallel_process_time.txt");
	#endif
	if (file_out.is_open()) {
		file_out << time;
		file_out.close();
	}	
}

void inline start_timing(struct rusage* usage, struct timeval* user_start, struct timeval* system_start) {
	getrusage(RUSAGE_SELF, usage); 
    *user_start = usage->ru_utime;
    *system_start = usage->ru_stime;	
}

void inline end_timing(struct rusage* usage, struct timeval* user_end, struct timeval* system_end) {
	getrusage(RUSAGE_SELF, usage); 
    *user_end = usage->ru_utime;
    *system_end = usage->ru_stime;	
}

void getTiming_User_System(double* timing, struct timeval* time_start1, struct timeval* time_end1,
										   struct timeval* time_start2, struct timeval* time_end2) {
	double aux;
	getTiming(&aux, time_start1, time_end1);
	getTiming(timing, time_start2, time_end2);
	*timing += aux;										   
}

int main() {	
	int m, k, n, cacheBlockSize, dataType;
	
	if (!readDimensions(&m, &k, &n, &cacheBlockSize, &dataType)) { 
		cout << "Couldn't read in.txt file!" << endl;
		return 0;	
	}	
	g_cacheBlockSize = cacheBlockSize;
	
	double read_time, process_time, write_time;
	struct timeval system_start, system_end, user_start, user_end; // Structs used by rusage
	struct rusage usage;
	struct timeval time_start, time_end; // Structs gettimeofday
	
	int single_core = 0; 
    int numThreads = std::thread::hardware_concurrency(); // Get number of cores
    
    // Timing start.
	start_timing(&usage, &user_start, &system_start);

	Array2D<E> matrix_A(0, 0);
	Array2D<E> matrix_B(0, 0);
	Array2D<E> matrix_C(0, 0);
	
	//Allocate matrices memory
	allocateMatrices(matrix_A, matrix_B, matrix_C, m, k, n);

	// Read Matrices A and B from input
    if (readInput(matrix_A, matrix_B, m, k, n) == 0) {
    	cout << "Couldn't open files to read matrices!" << endl;
    	return 0;
    }
	
	// Timing end.
	end_timing(&usage, &user_end, &system_end);
	
	// Get Time.
	getTiming_User_System(&read_time, &user_start, &user_end, &system_start, &system_end);
	
	// Write read time.
	writeReadTime(read_time);
    
    // Processing time start.
    #ifdef FORCE_SINGLE_CORE
    	start_timing(&usage, &user_start, &system_start);
    #else
    	gettimeofday(&time_start, NULL);
    #endif
		
	Thread thread[numThreads];
	
	#ifdef FORCE_SINGLE_CORE
    	single_core = 1;
    #endif
	
	// If can't split, size below or equal to block size, or single core forced.
	if ( (m <= g_mortonBlockSide || k <= g_mortonBlockSide || n <= g_mortonBlockSide) || 
		 (m == 1 || k == 1 || n == 1) || single_core) {
		// Standard optimized multiplication.
		matrixMultiplication(0, 0, m, k, 0, 0, n, 0, 0, matrix_A, matrix_B, matrix_C);	 	 
	} else {
		g_taskManager.initManager(numThreads);
		taskScheduler(0, 0, m, k, 0, 0, k, n, &matrix_A, &matrix_B, &matrix_C, thread, numThreads);
	}
	
	// Processing time end.
	#ifdef FORCE_SINGLE_CORE
    	end_timing(&usage, &user_end, &system_end);
    	getTiming_User_System(&process_time, &user_start, &user_end, &system_start, &system_end);
    #else
    	gettimeofday(&time_end, NULL);
    	getTiming(&process_time, &time_start, &time_end);
    #endif
	
	// Write processing time.
	writeProcessTime(process_time);
	
	// Timing start.
	start_timing(&usage, &user_start, &system_start);
	   
	writeOutput(matrix_C, m, n);
	
	// Timing end.
	end_timing(&usage, &user_end, &system_end);
	
	// Get Time.
	getTiming_User_System(&write_time, &user_start, &user_end, &system_start, &system_end);
	
	// Write writing time.
	writeWriteTime(write_time);

	
    return 0;
}
